{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aim of this notebook is to build the most basic rag pipeline to show the concept\n",
    "\n",
    "import openai\n",
    "import boto3\n",
    "import pprint\n",
    "from typing import Tuple\n",
    "\n",
    "boto3.setup_default_session(profile_name='conveyor-demo-profile') # not needed on conveyor [remove]\n",
    "\n",
    "BEDROCK_CLIENT = boto3.client('bedrock-agent-runtime', region_name='us-east-1') # note the region\n",
    "KNOWLEDGE_BASE_ID ='ECZYEUIJ59' # this is the id of the decisions knowledge base in english\n",
    "\n",
    "\n",
    "ssm_client = boto3.client('ssm', region_name='eu-west-1')\n",
    "open_ai_key = ssm_client.get_parameter(Name='llm-hackathon-openai-key')['Parameter']['Value']\n",
    "OPENAI_CLIENT = openai.OpenAI(\n",
    "    api_key=open_ai_key\n",
    ")\n",
    "\n",
    "\n",
    "GPT_4 = 'gpt-4-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper around the bedrock client retrieval method\n",
    "def retrieve(search_string, items=10):\n",
    "    retrievals = BEDROCK_CLIENT.retrieve(\n",
    "        knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "        retrievalQuery={\n",
    "            'text': search_string\n",
    "        },\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': items\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return retrievals['retrievalResults']\n",
    "\n",
    "\n",
    "# wrapper around the openai api\n",
    "def generate(prompt:str, model)->str:\n",
    "    api_response = OPENAI_CLIENT.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "    return api_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG function\n",
    "def RAG(query:str,model: str, num_retrivals:int)-> Tuple[str, str]:\n",
    "    \n",
    "    context_items = retrieve(query, num_retrivals)\n",
    "    flat_items = [f\"{index} : {item['content']['text']}\" for index, item in enumerate(context_items)]\n",
    "    flat_items = \"\\n\".join(flat_items)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "                Answer the given question using only the information in the sources section below.\n",
    "                Respond in a conversational manner, not with a list.\n",
    "                Enter the index number of the source used between single square brackets, such as [1] or [2][3][4].\n",
    "                do not combine references together in one bracket\n",
    "\n",
    "                question:{query}\n",
    "                sources: {flat_items}\n",
    "                your answer:\n",
    "    \"\"\"\n",
    "    return generate(prompt, model), flat_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are we doing to respond to climate change in belgium\"\n",
    "response, retrieved_items = RAG(question, GPT_4, 5)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(retrieved_items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

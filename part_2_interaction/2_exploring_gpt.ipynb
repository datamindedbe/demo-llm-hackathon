{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from typing import Tuple\n",
    "key = \"YOUR_KEY_HERE\"\n",
    "\n",
    "# some of the available models\n",
    "GPT_3 = 'gpt-3.5-turbo-0125' # 16,385 tokens\n",
    "GPT_4 = 'gpt-4-turbo' # 128,000 tokens\n",
    "GPT_4_TURBO = 'gpt-4-turbo' # 128,000 tokens\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper around the openai api\n",
    "def get_response(prompt:str, model)->Tuple[str, float]:\n",
    "    start_time = time.time()\n",
    "    api_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "    exec_time = time.time() - start_time\n",
    "    return (api_response.choices[0].message.content, exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model time comparison\n",
    "for model in [GPT_3, GPT_4, GPT_4_TURBO]:\n",
    "    response, run_time = get_response(\"tell me about belgium\", model)\n",
    "    print(response)\n",
    "    print(run_time)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but a big part in the time difference has to do with the verbosity of the response\n",
    "for model in [GPT_3, GPT_4, GPT_4_TURBO]:\n",
    "    response, run_time = get_response(\"tell me about belgium in just a few words\", model)\n",
    "    print(response)\n",
    "    print(run_time)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what can we do with prompts?\n",
    "response, run_time = get_response(\"what are the capital cities of western europe\", GPT_4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, run_time = get_response(\"what are the capital cities of western europe, respond only with a list and no added information\", GPT_4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, run_time = get_response(\"what are the capital cities of western europe, respond only with a list and no added information, structure the list like : INDEX: CITY [COUNTRY]\", GPT_4)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
